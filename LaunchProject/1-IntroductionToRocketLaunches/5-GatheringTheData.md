# Discover the Role Data Plays in Machine Learning

With a clear understanding of what you want to know, you can start to look at the data that you have, or the data that you want to start collecting. From there, you can prepare your data in a way that supports the discovery you're interested in.

## Step 2: Gather the Data

With constraints, scopes, and prioritazation of data based on subject matter expertise, the actual data can being to be collected. This can present it's own challenges. In the case of growing lettuce, you could have 10 heads of lettuce that you are trying to grow at the same time with slightly different conditions, and then determine which changes in conditions yielded the best results. 

In the case of rocket launches, comparison experiments are not as easy. While simulations can be made, simulations are based on data and data implies that there are rocket launches that are attempted under negative circumstances that failed (otherwise how would we *really* know that they are negative circumstances?). We can use other information (like from airplanes or basic physics) to determine constraints, but the issue is that it isn't ethical or economical to attempt under each unique circumstance to be able to determine with certainty what "good" circumstances are. Furthermore, the conditions are not able to be controlled because it is just natural weather. 

## Step 2.5: Clean and Manipulate the Data

The first time I heard that a step in machine learning was to "manipulate" the data I became suspicious, but it doesn't mean that the data is modified to get a desireable outcome. It means that the data is made to be the most accurate representation of the truth. 

For example, with out lettuce garden, we may want to run a study that focuses on the moisture in the soil. So we collect moisture readings every hour to be able to see what trends yield what results. But the moisture sensor broke on Tuesday at 2:55pm. We noticed and fixed it by the time the 4pm reading was taken, but we lost one sensor reading (the 3pm reading). It is reasonable to "manipulate" the data to take the average of the 2pm reading and 4pm reading and save that as the reading for missing 3pm time. Now let's say your sensor went out on Tuesday at 2:55pm but you didn't notice until Wednesday at 2:55pm. Then it might make more sense to "clean" your data by removing that day from the analysis all together.

In the case of rocket launches, there will be a lot of missing data for us, the learner. While NASA likely has a lot more data, because we do not have the subject matter expertise of all those involved in the nuances of rocket launches and weather, and because we don't have access to all of the previous experiments and analysis that NASA did, we will rely mostly on easily accessible weather data (temperature, percipication, cloudiness) and focus in on the days that launches happened. This will cause this project in particular to be less accurate than what NASA does realistically. The model we train will be skewed towards what good conditions look like, and not be great at identifying what bad conditions look like, because we only have examples of good conditions (successful launches).